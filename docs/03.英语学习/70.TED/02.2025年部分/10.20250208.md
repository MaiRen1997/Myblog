---
title: 20250208
date: 2025-02-10 19:28:41
permalink: /pages/8f1623/
categories:
  - 英语学习
  - TED
  - 2025年部分
tags:
  - 
author: 
  name: Riverside Joy
  link: https://github.com/MaiRen1997
---
This is something you won't like.
But here everyone is a **liar**.
**Don't take it too personally**.
**What I mean is that** lying is very common and it is now well-established that we lie on a daily basis.
**Indeed**, scientists have **estimated** that we **tell around two lies per day**,
although, of course, it's not that easy to establish those numbers with certainty.
And, well, I introduce myself.
I'm Riccardo, I'm a **psychologist** and a PhD candidate,
and for my research project I study **how good are people at detecting lies**.
Seems cool, right? But I'm not joking.
And you might wonder why a psychologist was then invited to give a TED Talk about AI.
And well, I'm here today because I'm about to tell you how AI could be used to detect lies.
And you will be very surprised by the answer.
But first of all, when is it relevant to detect lies?
A first clear example that comes to my mind
is in the **criminal** **investigation** **field**.
Imagine you are a police officer and you want to **interview** a **suspect**.
And the suspect is **providing some information to you**.
And this information is actually leading to the next steps of the **investigation**.
We certainly want to understand if the suspect is reliable or if they are trying to deceive us.
Then another example comes to my mind,
and I think this really affects all of us.
So please raise your hands
if you would like to know if your partner **cheated on** you.

cheat on sb 强调的是欺骗了谁

cheat with sb 强调的联合谁，欺骗了某人

(Laughter)
And don't be shy because I know.
(Laughter)
Yeah. You see?
It's very relevant.
However, **I have to  say that we as humans are very bad at detecting lies**.
In fact, many studies have already confirmed that when people are asked to judge
if someone is lying or not
without knowing much about that person or the context,
people's accuracy is no better than the chance level,
about the same as flipping a coin.
You might also wonder
if experts, such as police officers, **prosecutors**, experts
and even psychologists
are better at detecting lies.
And the answer is complex,
because experience alone doesn't seem to be enough
to help detect lies accurately.
It might help, but it's not enough.
To give you some numbers.
In a well-known **meta-analysis** that previous **scholars** did in 2006,
they found that **naive** judges' accuracy
was on average around 54 percent.
Experts perform only slightly better,
with an accuracy rate around 55 percent.
(Laughter)
Not that impressive, right?
And ...
Those numbers actually come from the analysis
of the results of 108 studies,
meaning that these findings are quite **robust**.
And of course, the debate is also much more **complicated** than this
and also more nuanced.

**complicated** vs **complex**

如果强调繁琐或难以处理，用 "more complicated"；

如果强调结构或内在关系复杂，用 "more complex"

But here the main **take-home** message
is that humans are not good at detecting lies.

take-home message" 强调这是听众或读者应该“带回家”（即记住并理解）的核心观点。

**What if we are creating an AI tool**
**where everyone can detect if someone else is lying?**
This is not possible yet, so please don't **panic**.
(Laughter)
But this is what we tried to do in a recent study
that I did together with my **brilliant** colleagues
whom I need to thank.
And actually, to let you understand what we did in our study,
I need to first introduce you to some technical concepts
and to the main **characters** of this story:
Large language models.
Large language models are AI systems
designed to generate outputs in natural language
**in a way** that almost **mimics** human communication.
If you are wondering how we teach these AI systems to detect lies,
here is where something called **fine-tuning** **comes in**.
But let's use a **metaphor**.
Imagine large language models being as students
who have **gone through** years of school,
learning a little bit about everything,
such as language, concepts, facts.
But when it's time for them to specialize,
like in law school or in medical school,
they need more focused training.
Fine-tuning is that extra education.
And of course, large language models don't learn as humans do.
But this is just to give you the main idea.
Then, as for training students, you need books, **lectures**, examples,
for training large language models you need **datasets**.
And for our study we considered three datasets,
one about personal opinions,
one about past **autobiographical** memories
and one about future **intentions**.
These datasets were already available from previous studies
and contained both **truthful** and **deceptive** statements.
Typically, you collect these types of statements
by asking **participants** to tell the truth or to lie about something.
For example, if I was a participant in the truthful condition,
and the task was
"tell me about your past holidays,"
then I will tell the researcher about my previous holidays in Vietnam,
and here we have a **slide** to **prove** it.
For the deceptive condition
they will randomly pick some of you who have never been to Vietnam,
and they will ask you to make up a story
and **convince** someone else that you've really been to Vietnam.
And this is how it typically works.
And as in all university courses, you might know this,
after **lectures** you have exams.
And **likewise** after training our AI models,
we would like to test them.
And the procedure that we followed,
that is actually the typical one, is the following.
So we picked some statements randomly from each dataset
and we **took them apart**.
So the model never saw these statements during the training phase.
And only after the training was completed,
we used them as a test, as the final exam.
But who was our student then?
In this case, it was a large language model
developed by Google
and called FLAN-T5.
Flanny, for friends.
And now that we have all the pieces of the process together,
we can actually **dig deep into** our study.
Our study was **composed** by three main **experiments**.
For the first **experiment**, we **fine-tuned** our model, our FLAN-T5,
on each single dataset **separately**.
For the second experiment,
we fine-tuned our model on two pairs of datasets together,
and we tested it on the third **remaining** one,
and we used all three possible combinations.
For the last final experiment,
we fine-tuned the model on a new, larger training test set
that we **obtained** by combining all the three datasets together.
The results were quite interesting
because what we found was that in the first experiment,
FLAN-T5 achieved an accuracy range between 70 percent and 80 percent.
However, in the second experiment,
FLAN-T5 dropped its accuracy to almost 50 percent.
And then, surprisingly, in the third experiment,
FLAN-T5 rose back to almost 80 percent.
But what does this mean?
What can we learn from these results?
From experiment one and three
we learn that language models
can effectively classify statements as deceptive,
**outperforming** human **benchmarks**
and **aligning with** previous machine learning
and deep learning models
that previous studies trained on the same datasets.
However, from the second experiment,
we see that language models **struggle**
in **generalizing** this knowledge, this learning across different contexts.
And this is **apparently** because
there is no one single **universal** rule of **deception**
that we can easily apply in every context,
but **linguistic** **cues** of deception are context-dependent.
And from the third experiment,
we learned that actually language models
can **generalize** well **across** different contexts,
if only they have <u>been **previously** exposed to examples</u>
during the training **phase**.
And I think this sounds as good news.
But while this means that language models can be effectively applied
for real-life applications in lie detection,
more **replication** is needed because a single study is never enough
so that from tomorrow we can all have these AI systems on our smartphones,
and start detecting other people's lies.
But as a scientist, I have a vivid imagination
and I would like to dream big.
And also I would like to **bring you with me** in this **futuristic** journey **for a while**.
So please imagine with me living in a world
where this lie detection technology is **well-integrated** in our life,
making everything from **national security** to social media a little bit safer.
And imagine having this AI system that could actually **spot** **fake** opinions.
From tomorrow, we could say
when a politician is actually saying one thing
and truly believes something else.
(Laughter)
And what about the security board context
where people are asked about their **intentions** and reasons
for why they are crossing borders or boarding planes.
Well, with these systems,
we could actually spot malicious intentions
before they even happen.
And what about the **recruiting** process?
(Laughter)
We heard about this already.
But actually, companies could **employ** this AI
to distinguish those who are really **passionate** about the role
from those who are just trying to say the right things to get the job.
And finally, we have social media.
Scammers trying to deceive you or to **steal** your **identity**.
**All gone**.
And someone else may claim something about **fake** news,
and well, perfectly, language model could automatically read the news,
flag them as **deceptive** or **fake**,
and we could even provide users with a **credibility** score
for the information they read.
It sounds like a **brilliant** future, right?
(Laughter)
Yes, but ...
**all great progress comes with risks**.
As much as I'm excited about this future,
I think we need to be careful.
If we are not cautious, in my view,
we could end up in a world
where people might just **blindly** believe AI outputs.
And I'm afraid this means that people will just be more likely
to **accuse** others of lying just because an AI says so.
And I'm not the only one with this view
because another study already **proved** it.
In addition, if we totally **rely on** this lie detection technology
to say someone else is lying or not,
**we risk losing** another important key value in society.
We lose trust.
We won't need to trust -people anymore,
because what we will do is just ask an AI to double check for us.
But are we really willing to blindly believe AI
and give up our **critical** thinking?
I think that's the future we need to avoid.
With hope for the future is more **interpretability**.
And I'm about to tell you what I mean.
Similar to when we look at **reviews** online,
and we can both look at the total number of stars at places,
but also we can look more in detail at the **positive and negative reviews**,
and try to understand what are the positive sides,
but also what might have gone wrong,
to eventually create our own and personal idea
if that is the place where we want to go,
where we want to be.
**Likewise**, imagine a world where AI doesn't just offer conclusions,
but also provides clear and understandable explanations
behind its decisions.
And I **envision** a future
where this lie detection technology
wouldn't just provide us with a simple **judgment**,
but also with clear explanations for why it thinks someone else is lying.
**And I would like a future where**, yes,
this lie detection technology is **integrated** in our life,
or also AI technology in **general**,
but still, at the same time,
we are able to think critically
and decide when we want to trust in AI judgment
or when we want to question it.
To conclude,
I think the future of using AI for lie detection
is not just about technological advancement,
but about enhancing our understanding and **fostering** trust.
It's about developing tools that don't replace human judgment
but **empower** it,
ensuring that we remain at the **helm**.
Don't step into a future with blind reliance on technology.
Let's commit to deep understanding and **ethical** use,
and we'll **pursue** the truth.
(Applause)
Thank you.

Machine Translation
这是你不会喜欢的事情。
但在这里每个人都是骗子。
别太介意。
我的意思是说谎非常常见，
"而且现在
我们每天都在说谎，这已是众所周知的事实。"
"事实上，科学家估计
我们每天大约会说两次谎，"
"尽管当然，要
确定这些数字并不容易。"
好吧，我来介绍一下自己。
"我是里卡多，一名心理学家
和博士生，"
"我的研究项目是研究
人们识别谎言的能力。"
看起来很酷，对吧？ 但我不是在开玩笑。
"你也许想知道
为什么一位心理学家会被邀请"
来做有关人工智能的 TED 演讲。
我今天来这里是
"为了告诉你们
如何使用人工智能来检测谎言。"
"你一定会对答案感到非常惊讶
。"
"但首先，
什么时候检测谎言才是重要的？ 我想到"
"第一个明显的例子
"
就是刑事调查领域。
"想象一下你是一名警察
，你想要采访一名嫌疑人。"
"并且犯罪嫌疑人正在
向你提供一些信息。"
"这些信息实际上引导了
调查的下一步行动。"
"我们当然想了解
嫌疑人是否可靠，"
或者他们是否试图欺骗我们。
然后我想到另一个例子，
我认为这确实影响到我们所有人。
所以，
"如果你想知道
你的伴侣是否欺骗了你，请举手。"
（笑声）
别害羞，我知道。
（笑声）
是的。 你看？
这非常有意义。
"然而，我不得不说，
我们人类"
很不擅长识别谎言。
"事实上，很多研究
已经证实，"
当人们在对对方或具体情况不太了解的情况下，判断
某人是否说谎时，
-
"人们的准确率并不
比偶然水平高，和"
抛硬币差不多。
您可能还想知道，
"警察、
检察官、专家"
甚至心理学家等专家是否
更善于识别谎言。
答案很复杂，
"因为单靠经验
似乎"
不足以准确地识别谎言。
这或许有帮助，但还不够。
给你一些数字。
"在2006年，学者们所做的一项著名的荟萃分析中
，"
他们发现天真的判断者的准确率
平均约为54%。
专家的表现仅略好一些，
准确率约为 55%。
（笑声）
没什么特别的，对吧？
而且...
"这些数字实际上
来自对"
108 项研究结果的分析，这
"意味着这些发现
非常可靠。"
"当然，争论
要比这复杂得多，"
也更加微妙。
但这里最主要的讯息
"是，人类并不
擅长察觉谎言。"
如果我们创建一种人工智能工具，让
"每个人都能检测出
其他人是否在说谎，那会怎样？"
"这目前还不可能，
所以请不要惊慌。"
（笑声）
"但这正是我们
在最近的研究中尝试做的，"
"我
和我的杰出的同事们一起做了这项研究，"
我需要向他们表示感谢。
"实际上，为了让你了解
我们在研究中所做的工作，"
"我需要首先向你介绍
一些技术概念"
和这个故事的主角：
大型语言模型。
大型语言模型是
"旨在以
"
"几乎模仿
人类交流的方式生成自然语言输出的人工智能系统。"
"如果你想知道我们如何教
这些人工智能系统检测谎言，"
"这就是所谓的
微调。"
但让我们打个比方。
"想象一下大型语言模型
就像"
经过多年学校教育的学生，他们
学习了各种知识，
例如语言、概念、事实。
但是当他们需要进入专业领域时，
比如进入法学院或医学院，
他们需要更有针对性的培训。
微调就是额外的教育。
"当然，大型语言模型的
学习方式并不像人类那样。"
"但这只是为了告诉你
主要思想。"
"然后，对于培训学生，
你需要书籍、讲座、例子；"
"对于训练大型语言模型，
你需要数据集。"
"在我们的研究中，
我们考虑了三个数据集，"
一个是关于个人观点，
一个是关于过去的自传体记忆
，一个是关于未来意图。
"这些数据集已经
从以前的研究中获得"
"，包含真实的
和欺骗性的陈述。"
"通常，你
"
"通过要求参与者说实话
或撒谎来收集这些类型的陈述。"
"例如，如果我是
真实条件下的参与者，"
并且任务是
“告诉我你过去的假期”，
"那么我会告诉研究人员
我以前在越南的假期，"
这里有一张幻灯片来证明这一点。
为了欺骗对方，
"他们会随机挑选一些
从未去过越南的人，"
要求你们编造一个故事
"，让别人相信
你真的去过越南。"
这就是它的典型工作方式。 你们可能知道，
"就像所有大学课程一样，
"
上完课之后还要参加考试。
同样，在训练我们的人工智能模型之后，
我们也想对其进行测试。
我们遵循的程序
"实际上是典型的程序，
如下所示。"
"因此，我们从每个数据集中随机挑选了一些语句，
"
然后将它们分开。
"因此模型在训练阶段从未见过这些陈述
。"
只有在培训完成后，
我们才将它们用作测试，作为期末考试。
但当时我们的学生是谁？
"在这种情况下，它是谷歌开发的
一个大型语言模型"
-
，称为 FLAN-T5。
弗兰尼，送给朋友。
"现在我们已经将所有
流程的部分整合在一起，"
可以深入研究了。
"我们的研究
由三个主要实验组成。"
"对于第一个实验，
我们分别在每个数据集上对我们的模型 FLAN-T5 进行了微调"
。
对于第二个实验，
"我们
在两对数据集上对模型进行了微调，"
"并
在剩下的第三对数据集上对其进行了测试，"
"并使用了所有三种
可能的组合。"
对于最后的最终实验，
"我们在通过将所有三个数据集组合在一起而获得的
新的、更大的训练测试集上对模型进行了微调"
"
。"
结果非常有趣，
"因为我们发现
在第一次实验中，"
"FLAN-T5 的准确率达到了
70% 到 80% 之间。"
然而，在第二次实验中，
"FLAN-T5 的准确率下降
到了近 50%。"
"然后，令人惊讶的是，
在第三次实验中，"
FLAN-T5 又回升至近 80%。
但这是什么意思呢？
我们可以从这些结果中了解到什么？
从实验一和实验三中，
我们了解到语言模型
"可以有效地将
语句分类为欺骗性语句，其表现"
优于人类基准，
"并与
"
-
"先前研究
在相同数据集上训练的机器学习和深度学习模型保持一致。"
然而，从第二个实验中，
我们看到语言模型很难
"将这些知识、
这种跨不同情境的学习概括化。"
这显然是因为
"不存在一条
"
可以轻易应用于所有情况的普遍欺骗规则，
"但欺骗的语言线索
却与具体情况有关。"
从第三个实验中，
我们了解到，实际上语言模型
"可以很好地推广
到不同的语境中，"
"只要它们
"
在训练阶段曾接触过示例。
我认为这听起来是个好消息。
"但虽然这意味着语言模型
可以有效地应用于"
"
谎言检测的实际应用，但"
"需要更多的复制，
因为单一的研究是远远不够的，"
"以便从明天开始我们都可以
在智能手机上拥有这些人工智能系统，"
并开始检测其他人的谎言。
"但作为一名科学家，
我拥有丰富的想象力，"
并且愿意去实现远大的梦想。
"我也想带你一起
踏上这段未来之旅。"
"所以请和我一起想象一下
生活在这样一个世界里，"
"谎言检测技术
已经很好地融入到我们的生活，"
"从国家安全
到社交媒体，一切都将变得更加安全。"
"想象一下，如果有一个人工智能系统
能够真正识别虚假观点。"
从明天起，我们就能知道
"什么时候政客
实际上是在说一套话，"
而什么时候他真正相信另一套话。
（笑声）
那么在安全公告板上，
"当人们被问及
"
"他们跨越边境
或登机的意图和原因时，情况又如何呢？"
有了这些系统，
"我们实际上可以在
恶意行为"
发生之前就发现它们。
那么招聘流程是怎样的呢？
（笑声）
我们已经听说过这件事了。
"但实际上，公司
可以使用这种人工智能"
"来区分那些
真正对职位充满热情的人和"
"那些只是为了
获得工作而说些正确的话的人。"
最后，我们有社交媒体。
"诈骗者试图欺骗您
或窃取您的身份。"
一切都消失了。
"而其他人可能会对
虚假新闻发表一些看法，"
"那么语言模型
就可以完美地自动阅读新闻，"
将其标记为欺骗性或虚假的，
"我们甚至可以为用户提供他们
"
所读信息的可信度分数。
这听起来像是一个美好的未来，对吧？
（笑声）
是的，但是……
所有伟大的进步都伴随着风险。
尽管我对这个未来感到兴奋，但
我认为我们需要小心。
在我看来，如果我们不谨慎的话，
我们最终可能会陷入一个
"人们
盲目相信人工智能输出的世界。"
"我担心这意味着
人们会更有可能"
"指责别人撒谎，
仅仅因为人工智能这么说。"
而且我不是唯一持有这种观点的人，
因为另一项研究已经证明了这一点。
"此外，如果我们完全
依赖这种测谎技术"
来判断别人是否撒谎，
"我们就有可能失去
社会中另一个重要的核心价值。"
我们失去了信任。
我们将不再需要信任人类，
"因为我们
只需让人工智能为我们再检查一遍。"
"但我们真的愿意
盲目相信人工智能"
并放弃批判性思维吗？
"我认为这就是
我们需要避免的未来。"
"希望未来
会有更多的可解释性。"
我接下来就要告诉你我的意思。 与
我们在网上查看评论时类似，
"我们既可以
查看各个地方的星级总数，"
"也可以更详细地查看
正面和负面的评论，"
"并尝试了解
哪些是积极的一面，"
哪些可能出了问题，
"最终形成
我们自己的想法，"
确定那是我们想去的地方、
我们想去的地方。
"同样，想象一下这样一个世界，
人工智能不仅能得出结论，"
"还能为其决策提供清晰
易懂的解释"
。
我设想的未来是，
这种测谎技术
"不仅能为我们提供
简单的判断，"
"还能清楚地解释
为什么它认为别人在说谎。"
我希望在未来，
"测谎技术或者人工智能技术
能够融入我们的生活"
，
但与此同时，
我们仍然能够批判性地思考
"，决定何时应该
相信人工智能的判断"
，何时应该质疑它。
总而言之，
"我认为未来使用人工智能
进行测谎"
"不仅关乎
技术进步，"
"还关乎增进我们的理解
和培养信任。"
"这是为了开发一些工具，
它们不会取代人类的判断力，"
而是会增强人类的判断力，
确保我们能够掌控局势。
"不要
盲目依赖科技走进未来。"
"让我们致力于深刻理解
和道德使用，"
追求真理。
（掌声）
谢谢。