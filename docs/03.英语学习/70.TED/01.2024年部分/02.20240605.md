---
title: 20240605
date: 2024-06-05 22:26:38
permalink: /pages/b78e98/
author: Riverside Joy
categories:
  - 英语学习
  - TED
  - 2024年部分
tags:
  - 
---
I want to tell you a story	我想给大家讲一个

about artificial intelligence and farmers.	关于人工智能和农民的故事。

Now, what a strange combination, right?	现在，这是多么奇怪的组合，对吧？

Two topics could not sound more different from each other.	两个主题听起来截然不同。

But did you know that modern farming actually involves a lot of technology?	但你知道现代农业实际上涉及很多技术吗？

So computer vision is used to predict crop yields.	因此计算机视觉被用来预测农作物产量。

And artificial intelligence is used to find, identify and get rid of insects.	人工智能用于发现、识别和消灭昆虫。

Predictive analytics helps figure out extreme weather conditions like drought or hurricanes.	预测分析有助于找出干旱或飓风等极端天气条件。

But this technology is also alienating to farmers.	但这项技术也让农民感到陌生。

And this all came to a head in 2017 with the tractor company John Deere when they introduced smart tractors.	2017 年，拖拉机公司约翰迪尔 (John Deere)推出了智能拖拉机，这一切都达到了顶峰。

So before then, if a farmer's tractor broke, they could just repair it themselves or take it to a mechanic.	"所以在此之前，如果农民的拖拉机坏了，他们只能自己修理或交给机械师。

Well, the company actually made it illegal for farmers to fix their own equipment.	嗯，该公司实际上将农民自己修理设备定为非法。
You had to use a licensed technician and farmers would have to wait for weeks while their crops rot and pests took over.	你必须使用有执照的技术人员，农民们必须等待数周，直到他们的农作物腐烂和害虫蔓延。

So they took matters into their own hands.	所以他们把事情掌握在自己手中。

Some of them learned to program, and they worked with hackers to create patches to repair their own systems.	他们中的一些人学会了编程，并与黑客合作创建补丁来修复自己的系统。

In 2022, at one of the largest hacker conferences in the world, DEFCON, a hacker named Sick Codes and his team showed everybody how to break into a John Deere tractor, showing that, first of all, the technology was vulnerable, but also that you can and should own your own equipment.	2022 年，在世界上最大的黑客会议之一 DEFCON 上，一位名叫 Sick Codes 的黑客和他的团队向大家展示了如何闯入约翰迪尔拖拉机，这表明，首先，该技术是脆弱的，但也表明 您可以而且应该拥有自己的设备。

To be clear, this is illegal, but there are people trying to change that.	需要明确的是，这是非法的，但有人试图改变这一点。

Now that movement is called the “right to repair.”	现在，这项运动被称为“修复权”。"

The right to repair goes something like this.	修理权是这样的。

If you own a piece of technology, it could be a tractor, a smart toothbrush, a washing machine, you should have the right to repair it if it breaks.	如果你拥有一项技术，它可能是拖拉机、智能牙刷、洗衣机，如果它坏了，你应该有权修理它。

So why am I telling you this story?	那么我为什么要告诉你这个故事呢？

The right to repair needs to extend to artificial intelligence.	修复权需要延伸至人工智能。

Now it seems like every week there is a new and mind-blowing innovation in AI.	现在，人工智能领域似乎每周都会出现令人兴奋的新创新。

But did you know that public confidence is actually declining?	但你知道公众信心实际上正在下降吗？

A recent Pew poll showed that more Americans are concerned than they are excited about the technology.	"皮尤研究中心最近的一项民意调查显示，对这项技术感到担忧的美国人多于兴奋的美国人

This is echoed throughout the world.	这在世界各地得到回应。

The World Risk Poll shows that respondents from Central and South America and Africa all said that they felt AI would lead to more harm than good for their people.	世界风险民意调查显示，来自中南美洲和非洲的受访者均表示，他们认为人工智能会给他们的人民带来弊大于利。

As a social scientist and an AI developer, this frustrates me.	作为一名社会科学家和人工智能开发人员，这让我感到沮丧。

I'm a tech optimist because I truly believe this technology can lead to good.	我是一名技术乐观主义者，因为我坚信这项技术可以带来好处。

So what's the disconnect?	那么脱节是什么？

Well, I've talked to hundreds of people over the last few years.嗯，过去几年我已经和数百人交谈过。

Architects and scientists, journalists and photographers, ride-share drivers and doctors, and they all say the same thing.	"建筑师和科学家、记者和摄影师、拼车司机和医生，他们都说同样的话。

People feel like an afterthought.	人们感觉像是事后诸葛亮。

They all know that their data is harvested often without their permission to create these sophisticated systems.	他们都知道，他们的数据经常在未经他们许可的情况下被收集来创建这些复杂的系统。

They know that these systems are determining their life opportunities.	他们知道这些系统正在决定他们的生活机会。

They also know that nobody ever bothered to ask them how the system should be built, and they certainly have no idea where to go if something goes wrong.	他们还知道，没有人费心去问他们应该如何构建系统，而且他们当然不知道如果出现问题该去哪里。

We may not own AI systems, but they are slowly dominating our lives.	我们可能没有人工智能系统，但它们正在慢慢主宰我们的生活。

We need a better feedback loop between the people who are making these systems, and the people who are best determined to tell us how these AI systems should interact in their world.	我们需要在制造这些系统的人和最有
决心告诉我们这些人工智能系统应该如何在他们的世界中互动的人之间建立更好的反馈循环。

One step towards this is a process called red teaming.	"实现这一目标的一个步骤是称为红队的过程。

Now, red teaming is a practice that was started in the military, and it's used in cybersecurity.	现在，红队是一种从军队开始的做法，并用于网络安全。

In a traditional red-teaming exercise, external experts are brought in to break into a system, sort of like what Sick Codes did with tractors, but legal.	在传统的红队演习中，外部专家被请来闯入系统，有点像 Sick Codes对拖拉机所做的那样，但合法。

So red teaming acts as a way of testing your defenses and when you can figure out where something will go wrong, you can figure out how to fix it.	因此，红队是测试你的防御能力的一种方式，当你能找出哪里出了问题时，你就能找出解决方法。

But when AI systems go rogue, it's more than just a hacker breaking in.	但当人工智能系统失控时，这不仅仅是黑客闯入的问题。

The model could malfunction or misrepresent reality.	该模型可能会发生故障或歪曲现实。

So, for example, not too long ago, we saw an AI system attempting diversity by showing historically inaccurate photos.	例如，不久前，我们看到一个人工智能系统通过显示历史上不准确的照片来尝试多样性。

Anybody with a basic understanding of Western history could have told you that neither the Founding Fathers nor Nazi-era soldiers would have been Black.	任何对西方历史有基本了解的人都会告诉你，开国元勋和纳粹时代的士兵都不会是黑人。 
In that case, who qualifies as an expert?	那么，谁有资格成为专家呢？

You.	你。

I'm working with thousands of people all around the world on large and small red-teaming exercises, and through them we found and fixed mistakes in AI models.	我正在与世界各地的数千人合作进行大大小小的红队练习，通过他们我们发现并修复了人工智能模型中的错误。

We also work with some of the biggest tech companies in the world: OpenAI, Meta, Anthropic, Google.	OpenAI、Meta、Anthropic、Google。	我们还与世界上一些最大的科技公司合作：

And through this, we've made models work better for more people.	通过这一点，我们让模型更好地为更多人服务。

Here's a bit of what we've learned.	这是我们学到的一些内容。

We partnered with the Royal Society in London to do a scientific, mis- and disinformation event with disease scientists.	我们与伦敦皇家学会合作，与疾病科学家一起举办了一场科学、错误和虚假信息活动。
What these scientists found is that AI models actually had a lot of protections against COVID misinformation.	这些科学家发现，人工智能模型实际上对新冠错误信息有很多保护措施。

But for other diseases like measles, mumps and the flu, the same protections didn't apply.	"但对于麻疹、腮腺炎和流感等其他疾病，同样的保护措施并不适用。

We reported these changes, they’re fixed and now we are all better protected against scientific mis- and disinformation.	我们报告了这些变化，它们已得到修复，现在我们都得到了更好的保护，免受科学错误和虚假信息的侵害。

We did a really similar exercise with architects at Autodesk University, and we asked them a simple question:	"我们与欧特克大学的建筑师进行了一次非常类似的练习，我们问了他们一个简单的问题：

Will AI put them out of a job?	人工智能会让他们失业吗？

Or more specifically, could they imagine a modern AI system that would be able to design the specs of a modern art museum?	或者更具体地说，他们能否想象一个能够设计现代艺术博物馆规格的现代人工智能系统？

The answer, resoundingly, was no.	答案是否定的。

Here's why, architects do more than just draw buildings.	这就是为什么建筑师所做的不仅仅是绘制建筑物。

They have to understand physics and material science.	他们必须了解物理学和材料科学。

They have to know building codes, and they have to do that while making something that evokes emotion.	他们必须了解建筑规范，并且必须在制作能够唤起情感的东西的同时做到这一点。

What the architects wanted was an AI system that interacted with them, that would give them feedback, maybe proactively offer design recommendations.	建筑师想要的是一个能够与他们交互的人工智能系统，可以给他们反馈，也许可以主动提供设计建议。

And today's AI systems, not quite there yet.	而今天的人工智能系统还没有完全实现。

But those are technical problems.	但这些都是技术问题。

People building AI are incredibly smart, and maybe they could solve all that in a few years.	构建人工智能的人非常聪明，也许他们可以在几年内解决所有问题。

But that wasn't their biggest concern.	但这并不是他们最关心的问题。

Their biggest concern was trust.	他们最关心的是信任。

Now architects are liable if something goes wrong with their buildings.	现在，如果他们的建筑出现问题，建筑师就要承担责任。

They could lose their license, they could be fined, they could even go to prison.	他们可能会被吊销执照，可能会被罚款，甚至可能会入狱。

And failures can happen in a million different ways.	失败可能以一百万种不同的方式发生。

For example, exit doors that open the wrong way, leading to people being crushed in an evacuation crisis, or broken glass raining down onto pedestrians in the street because the wind blows too hard and shatters windows.	例如，出口门以错误的方式打开，导致人们在疏散危机中被压伤，或者由于风吹得太猛，窗户被吹碎，碎玻璃如雨点般落到街上的行人身上。

So why would an architect trust an AI system with their job, with their literal freedom, if they couldn't go in and fix a mistake if they found it?	那么，如果建筑师在发现错误时无法介入并修复错误，为什么他们会信任人工智能系统来完成他们的工作和真正的自由呢？

So we need to figure out these problems today, and I'll tell you why.	所以我们今天需要弄清楚这些问题，我会告诉你原因。

The next wave of artificial intelligence systems, called agentic AI, is a true tipping point between whether or not we retain human agency, or whether or not AI systems make our decisions for us.	下一波人工智能系统被称为代理人工智能，它是
我们是否保留人类代理或人工智能系统是否为我们做出决定之间的真正转折点。

Imagine an AI agent as kind of like a personal assistant.	想象一下人工智能代理有点像私人助理。

So, for example, a medical agent might determine whether or not your family needs doctor's appointments, it might refill prescription medications, or in case of an emergency, send medical records to the hospital.	例如，医疗代理人可能会确定您的家人是否需要预约医生，可能会补充处方药，或者在紧急情况下将医疗记录发送到医院。

But AI agents can't and won't exist unless we have a true right to repair.	但除非我们拥有真正的修复权，否则人工智能代理不能也不可能存在。 除非你能进行一些基本的诊断，否则

What parent would trust their child's health to an AI system unless you could run some basic diagnostics?		哪个父母会把孩子的健康托付给人工智能系统呢？ 除非你能像初级员工那样重新培训它，否则

What professional would trust an AI system with job decisions, unless you could retrain it the way you might a junior employee?	哪个专业人士会信任人工智能系统来做出工作决策呢？

Now, a right to repair might look something like this.	"现在，修复权看起来像这样。

You could have a diagnostics board where you run basic tests that you design, and if something's wrong, you could report it to the company and hear back when it's fixed.	您可以有一个诊断板，您可以在其中运行您设计的基本测试，如果出现问题，您可以向公司报告并在修复后收到回复。

Or you could work with third parties like ethical hackers who make patches for systems like we do today.	"或者您可以与第三方合作，例如道德黑客，他们像我们今天一样为系统制作补丁。

You can download them and use them to improve your system the way you want it to be improved.	您可以下载它们并使用它们按照您希望的方式改进您的系统。

Or you could be like these intrepid farmers and learn to program and fine-tune your own systems.	"或者您可以像这些勇敢的农民一样，学习编程和微调自己的系统。

We won't achieve the promised benefits of artificial intelligence unless we figure out how to bring people into the development process.	除非我们弄清楚如何让人们参与开发过程，否则我们无法实现人工智能所承诺的好处。

I've dedicated my career to responsible AI, and in that field we ask the question, what can companies build to ensure that people trust AI?	"我将我的职业生涯奉献给了负责任的人工智能，在这个领域我们提出这样的问题：公司可以构建什么
来确保人们信任人工智能？

Now, through these red-teaming exercises, and by talking to you, I've come to realize that we've been asking the wrong question all along.	"现在，通过这些红队练习以及与您的交谈，我逐渐意识到我们一直在问错误的问题。

What we should have been asking is what tools can we build so people can make AI beneficial for them?	我们应该问的是，我们可以构建哪些工具，以便人们可以让人工智能为他们带来好处？

Technologists can't do it alone.	技术人员无法单独做到这一点。

We can only do it with you.	我们只能和你一起做。

Thank you.	谢谢。

(Applause)	（掌声）