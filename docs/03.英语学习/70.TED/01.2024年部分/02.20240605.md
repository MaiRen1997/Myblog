---
title: 20240605
date: 2024-06-05 22:26:38
permalink: /pages/b78e98/
author: Riverside Joy
categories:
  - 英语学习
  - TED
  - 2024年部分
tags:
  - 
---
I want to tell you a story about artificial intelligence and farmers.
Now, what a strange combination, right?
Two topics could not sound more different from each other.
But did you know that modern farming actually involves a lot of technology?
So computer vision is used to predict crop yields.
And artificial intelligence is used to find,
identify and get rid of insects.
Predictive analytics helps figure out extreme weather conditions
like drought or hurricanes.
But this technology is also alienating to farmers.
And this all came to a head in 2017
with the tractor company John Deere when they introduced smart tractors.
So before then, if a farmer's tractor broke,
they could just repair it themselves or take it to a mechanic.
Well, the company actually made it illegal
for farmers to fix their own equipment.
You had to use a licensed technician
and farmers would have to wait for weeks
while their crops rot and pests took over.
So they took matters into their own hands.
Some of them learned to program,
and they worked with hackers to create patches to repair their own systems.
In 2022,
at one of the largest hacker conferences in the world, DEFCON,
a hacker named Sick Codes and his team
showed everybody how to break into a John Deere tractor,
showing that, first of all, the technology was vulnerable,
but also that you can and should own your own equipment.
To be clear, this is illegal,
but there are people trying to change that.
Now that movement is called the “right to repair.”
The right to repair goes something like this.
If you own a piece of technology,
it could be a tractor, a smart toothbrush,
a washing machine,
you should have the right to repair it if it breaks.
So why am I telling you this story?
The right to repair needs to extend to artificial intelligence.
Now it seems like every week
there is a new and mind-blowing innovation in AI.
But did you know that public confidence is actually declining?
A recent Pew poll showed that more Americans are concerned
than they are excited about the technology.
This is echoed throughout the world.
The World Risk Poll shows
that respondents from Central and South America and Africa
all said that they felt AI would lead to more harm than good for their people.
As a social scientist and an AI developer,
this frustrates me.
I'm a tech optimist
because I truly believe this technology can lead to good.
So what's the disconnect?
Well, I've talked to hundreds of people over the last few years.
Architects and scientists, journalists and photographers,
ride-share drivers and doctors,
and they all say the same thing.
People feel like an afterthought.
They all know that their data is harvested often without their permission
to create these sophisticated systems.
They know that these systems are determining their life opportunities.
They also know that nobody ever bothered to ask them
how the system should be built,
and they certainly have no idea where to go if something goes wrong.
We may not own AI systems,
but they are slowly dominating our lives.
We need a better feedback loop
between the people who are making these systems,
and the people who are best determined to tell us
how these AI systems should interact in their world.
One step towards this is a process called red teaming.
Now, red teaming is a practice that was started in the military,
and it's used in cybersecurity.
In a traditional red-teaming exercise,
external experts are brought in to break into a system,
sort of like what Sick Codes did with tractors, but legal.
So red teaming acts as a way of testing your defenses
and when you can figure out where something will go wrong,
you can figure out how to fix it.
But when AI systems go rogue,
it's more than just a hacker breaking in.
The model could malfunction or misrepresent reality.
So, for example, not too long ago,
we saw an AI system attempting diversity
by showing historically inaccurate photos.
Anybody with a basic understanding of Western history
could have told you that neither the Founding Fathers
nor Nazi-era soldiers would have been Black.
In that case, who qualifies as an expert?
You.
I'm working with thousands of people all around the world
on large and small red-teaming exercises,
and through them we found and fixed mistakes in AI models.
We also work with some of the biggest tech companies in the world:
OpenAI, Meta, Anthropic, Google.
And through this, we've made models work better for more people.
Here's a bit of what we've learned.
We partnered with the Royal Society in London to do a scientific,
mis- and disinformation event with disease scientists.
What these scientists found
is that AI models actually had a lot of protections
against COVID misinformation.
But for other diseases like measles, mumps and the flu,
the same protections didn't apply.
We reported these changes,
they’re fixed and now we are all better protected
against scientific mis- and disinformation.
We did a really similar exercise with architects at Autodesk University,
and we asked them a simple question:
Will AI put them out of a job?
Or more specifically,
could they imagine a modern AI system
that would be able to design the specs of a modern art museum?
The answer, resoundingly, was no.
Here's why, architects do more than just draw buildings.
They have to understand physics and material science.
They have to know building codes,
and they have to do that
while making something that evokes emotion.
What the architects wanted was an AI system
that interacted with them, that would give them feedback,
maybe proactively offer design recommendations.
And today's AI systems, not quite there yet.
But those are technical problems.
People building AI are incredibly smart,
and maybe they could solve all that in a few years.
But that wasn't their biggest concern.
Their biggest concern was trust.
Now architects are liable if something goes wrong with their buildings.
They could lose their license,
they could be fined, they could even go to prison.
And failures can happen in a million different ways.
For example, exit doors that open the wrong way,
leading to people being crushed in an evacuation crisis,
or broken glass raining down onto pedestrians in the street
because the wind blows too hard and shatters windows.
So why would an architect trust an AI system with their job,
with their literal freedom,
if they couldn't go in and fix a mistake if they found it?
So we need to figure out these problems today, and I'll tell you why.
The next wave of artificial intelligence systems, called agentic AI,
is a true tipping point
between whether or not we retain human agency,
or whether or not AI systems make our decisions for us.
Imagine an AI agent as kind of like a personal assistant.
So, for example, a medical agent might determine
whether or not your family needs doctor's appointments,
it might refill prescription medications, or in case of an emergency,
send medical records to the hospital.
But AI agents can't and won't exist
unless we have a true right to repair.
What parent would trust their child's health to an AI system
unless you could run some basic diagnostics?
What professional would trust an AI system with job decisions,
unless you could retrain it the way you might a junior employee?
Now, a right to repair might look something like this.
You could have a diagnostics board
where you run basic tests that you design,
and if something's wrong, you could report it to the company
and hear back when it's fixed.
Or you could work with third parties like ethical hackers
who make patches for systems like we do today.
You can download them and use them to improve your system
the way you want it to be improved.
Or you could be like these intrepid farmers and learn to program
and fine-tune your own systems.
We won't achieve the promised benefits of artificial intelligence
unless we figure out how to bring people into the development process.
I've dedicated my career to responsible AI,
and in that field we ask the question,
what can companies build to ensure that people trust AI?
Now, through these red-teaming exercises, and by talking to you,
I've come to realize that we've been asking the wrong question all along.
What we should have been asking is what tools can we build
so people can make AI beneficial for them?
Technologists can't do it alone.
We can only do it with you.
Thank you.
(Applause)

我想给大家讲一个
关于人工智能和农民的故事。
现在，这是多么奇怪的组合，对吧？
"两个主题听起来
截然不同。"
"但你知道现代农业
实际上涉及很多技术吗？"
"因此计算机视觉被用来
预测农作物产量。"
"人工智能
用于发现、"
识别和消灭昆虫。
"预测分析有助于找出
"
干旱或飓风等极端天气条件。
"但这项技术
也让农民感到陌生。"
2017 年，
"拖拉机公司约翰迪尔 (John Deere)
推出了智能拖拉机，这一切都达到了顶峰。"
"所以在此之前，
如果农民的拖拉机坏了，"
"他们只能自己修理
或交给机械师。"
嗯，该公司实际上将
农民自己修理设备定为非法。
你必须使用有执照的技术人员，
农民们必须等待数周，
直到他们的农作物腐烂和害虫蔓延。
所以他们把事情掌握在自己手中。
他们中的一些人学会了编程，
"并与黑客合作创建
补丁来修复自己的系统。"
2022 年，在世界上
"最大的黑客
会议之一 DEFCON 上，"
一位名叫 Sick Codes 的黑客和他的团队向
"所有人展示了如何闯
入约翰迪尔拖拉机，这"
"表明，首先，
该技术是脆弱的，"
"但也表明 您可以而且应该
拥有自己的设备。"
需要明确的是，这是非法的，
"但有人
试图改变这一点。"
"现在，这项运动被称为
“修复权”。"
"修理权
是这样的。"
如果你拥有一项技术，
它可能是拖拉机、智能牙刷、
洗衣机，
"
如果它坏了，你应该有权修理它。"
那么我为什么要告诉你这个故事呢？
"修复权需要延伸
至人工智能。"
现在，人工智能领域似乎每周都会出现
"令人兴奋的新
创新。"
"但你知道公众信心
实际上正在下降吗？"
"皮尤研究中心最近的一项民意调查显示，对这项技术感到
担忧的美国人"
"多于兴奋的美国人
。"
这在世界各地得到回应。
世界风险民意调查显示，
"来自
中南美洲和非洲的受访"
"者均表示，他们认为人工智能会给
他们的人民带来弊大于利。"
作为一名社会科学家和人工智能开发人员，
这让我感到沮丧。
我是一名技术乐观主义者，
"因为我坚信
这项技术可以带来好处。"
那么脱节是什么？
"嗯，过去几年我已经和数百人交谈过
。"
"建筑师和科学家、
记者和摄影师、"
拼车司机和医生，
他们都说同样的话。
人们感觉像是事后诸葛亮。
"他们都知道，他们的数据
经常在未经他们许可的情况下被收集来"
创建这些复杂的系统。
"他们知道这些系统
正在决定他们的生活机会。"
"他们还知道，没有人
费心去问他们"
应该如何构建系统，
"而且他们当然不知道
如果出现问题该去哪里。"
我们可能不拥有人工智能系统，
但它们正在慢慢主宰我们的生活。
我们需要在
"
制造这些系统的人和"
"最有
决心告诉我们"
"这些人工智能系统
应该如何在他们的世界中互动的人之间建立更好的反馈循环。"
"实现这一目标的一个步骤
是称为红队的过程。"
"现在，红队是一种
从军队开始的做法，"
并用于网络安全。
在传统的红队演习中，
"外部专家被请来
闯入系统，"
"有点像 Sick Codes
对拖拉机所做的那样，但合法。"
"因此，红队是
测试你的防御能力的一种方式"
"，当你能找出
哪里出了问题时，"
你就能找出解决方法。
但当人工智能系统失控时，
这不仅仅是黑客闯入的问题。
"该模型可能会发生故障
或歪曲现实。"
例如，不久前，
我们看到一个人工智能系统
通过显示历史上不准确的照片来尝试多样性。
"任何对
西方历史有基本了解的人"
"都会告诉你
，开国元勋"
"和纳粹时代的士兵
都不会是黑人。 那么"
，谁有资格成为专家呢？
你。
"我正在与世界各地的数千人合作进行
"
大大小小的红队练习，
"通过他们我们发现
并修复了人工智能模型中的错误。"
"我们还与世界上一些最大的
科技公司合作："
OpenAI、Meta、Anthropic、Google。
"通过这一点，我们让模型
更好地为更多人服务。"
这是我们学到的一些内容。
"我们与伦敦皇家学会合作，与疾病科学家一起
举办了一场科学、"
"错误和虚假信息活动
。"
这些科学家发现，
"人工智能模型实际上
"
对新冠错误信息有很多保护措施。
"但对于麻疹、
腮腺炎和流感等其他疾病，"
同样的保护措施并不适用。
我们报告了这些变化，
"它们已得到修复，现在
我们都得到了更好的保护，"
"免受科学错误
和虚假信息的侵害。"
"我们与欧特克大学的建筑师进行了非常类似的练习
，"
我们问他们一个简单的问题：
人工智能会让他们失业吗？
或者更具体地说，
他们能否想象一个
"能够设计
现代艺术博物馆规格的现代人工智能系统？"
答案是否定的。
"这就是为什么建筑师所做的
不仅仅是绘制建筑物。"
"他们必须了解物理学
和材料科学。"
他们必须了解建筑规范，
并且必须在
"制作能够
唤起情感的东西的同时做到这一点。"
"建筑师想要的
是一个能够"
"与他们交互的人工智能系统，
可以给他们反馈，"
"也许可以主动提供
设计建议。"
"而今天的人工智能系统
还没有完全实现。"
但这些都是技术问题。
构建人工智能的人非常聪明，
"也许他们可以
在几年内解决所有问题。"
但这并不是他们最关心的问题。
他们最关心的是信任。
"现在，如果
他们的建筑出现问题，建筑师就要承担责任。"
他们可能会被吊销驾照，
"可能会被罚款，甚至
可能会入狱。"
"失败可能
以一百万种不同的方式发生。"
"例如，出口门
打开方向错误，"
"导致人们
在疏散危机中被压伤，"
"或者
"
"由于风吹得太猛，
窗户被吹碎，碎玻璃如雨点般落到街上的行人身上。"
"那么，如果建筑师在发现错误时无法介入并修复错误，为什么他们会信任
人工智能系统来完成他们的工作和"
真正的自由呢
"
？"
"所以我们今天需要弄清楚这些问题
，我会告诉你原因。"
"下一波人工智能
系统被称为代理人工智能，它"
是
"
我们是否保留人类代理"
"或人工智能系统是否
为我们做出决定之间的真正转折点。"
"想象一下人工智能代理有点
像私人助理。"
"例如，
医疗代理人可能会确定"
"您的家人是否需要
预约医生，"
"可能会补充处方药，
或者在紧急情况下将"
医疗记录发送到医院。
但
除非我们拥有真正的修复权，否则人工智能代理不能也不可能存在。 除非你能进行一些基本的诊断，否则
"哪个父母会把
孩子的健康托付给人工智能系统呢"
"
？ 除非你能像初级员工那样重新培训它，否则"
"哪个专业人士会信任
人工智能系统来做出工作决策呢"
"
？"
"现在，修复权
可能看起来像这样。"
您可以有一个诊断板，
您可以在其中运行您设计的基本测试，
"如果出现问题，
您可以向公司报告"
并在修复后收到回复。
"或者您可以与第三方合作，
例如道德黑客，"
"他们像我们今天一样为系统制作补丁
。"
"您可以下载它们并使用它们
"
按照您希望的方式改进您的系统。
"或者您可以像这些勇敢的
农民一样，学习编程"
和微调自己的系统。
"除非我们弄清楚如何让人们
参与开发过程，否则我们无法实现人工智能所承诺的好处。"
"我将我的职业生涯奉献
给了负责任的人工智能，"
在这个领域我们提出这样的问题：
"公司可以构建什么
来确保人们信任人工智能？"
"现在，通过这些红队练习
以及与您的交谈，"
"我逐渐意识到我们一直在
问错误的问题。"
"我们应该问的
是，我们可以构建哪些工具，"
以便人们可以让人工智能为他们带来好处？
技术人员无法单独做到这一点。
我们只能和你一起做。
谢谢。
（掌声）